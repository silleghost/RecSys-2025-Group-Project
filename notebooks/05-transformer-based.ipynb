{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14238413,"sourceType":"datasetVersion","datasetId":9084054}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport math\n\nimport pandas as pd\n\npath = '/kaggle/input/steam-data/'\n\ntrain_interactions = pd.read_parquet(path + 'train_interactions.parquet')\ntest_interactions = pd.read_parquet(path + 'test_interactions.parquet')\n\ngames_metadata = pd.read_parquet(path + 'games_metadata.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:11:15.802788Z","iopub.execute_input":"2025-12-21T02:11:15.803109Z","iopub.status.idle":"2025-12-21T02:11:20.800437Z","shell.execute_reply.started":"2025-12-21T02:11:15.803080Z","shell.execute_reply":"2025-12-21T02:11:20.799611Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## SBERT + KNN","metadata":{}},{"cell_type":"markdown","source":"### Game Metadata Preprocessing (Content-Based Features)\n\nThis part of the pipeline is responsible for transforming raw Steam data into semantic vectors. The primary goal is to combine textual information, numerical metrics, and technical specifications into a unified vector space.\n\n#### Key Processing Stages:\n\n1. **Text Engineering (NLP):**\n    * **Feature Concatenation:** Fields such as `name`, `tags`, `genres`, and `short_description` are combined into a single string. This allows the model to capture the full context of a game (e.g., combining title keywords with genre-specific tags).\n    * **SBERT Embeddings:** We utilize the **SBERT** (`paraphrase-multilingual-MiniLM-L12-v2`) pre-trained transformer. It understands semantic meaning across multiple languages and encodes the text into a fixed-length dense vector.\n\n2. **Numerical Feature Processing:**\n    * **Cleaning:** Handling infinite values (`inf`) and missing data (`NaN`) resulting from data scraping inconsistencies.\n    * **Log-Transformation:** Applying the $\\ln(1+x)$ function to features like review counts and playtime. This is essential for normalizing data with extreme variance (ranging from zero to millions).\n    * **Scaling:** Bringing all numerical values to a common scale using `StandardScaler` to ensure stability during neural network training.\n\n\n\n3. **Technical Specifications:**\n    * Converting operating system support flags (Windows, Mac, Linux) into a binary format (1 or 0).\n\n#### Final Vector Structure:\nThe output of this process is a feature matrix where each game is represented by a concatenated vector:\n$$V_{game} = [E_{sbert} \\parallel F_{numeric} \\parallel F_{os}]$$\n\n* $E_{sbert}$ — Semantic vector (384 dimensions).\n* $F_{numeric}$ — Scaled indicators (price, reviews, metacritic scores).\n* $F_{os}$ — Binary platform features.\n\n\n\n#### Final Output:\nThe function returns a `game_map` dictionary where the key is the `appid` and the value is the multidimensional feature vector. This dictionary acts as the model's \"memory,\" defining the characteristics of every game in the dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_metadata(df):\n    df = df.copy()\n    \n    # 1. Очистка текста\n    df['short_description'] = df['short_description'].fillna('')\n    df['name'] = df['name'].fillna('Unknown Game')\n    \n    def list_to_str(x):\n        if isinstance(x, (list, np.ndarray)):\n            return \", \".join(map(str, x))\n        return str(x) if pd.notnull(x) else \"\"\n\n    print(\"Генерация текста...\")\n    df['combined_text'] = (\n        df['name'] + \". Tags: \" + \n        df['tags'].apply(list_to_str) + \". Genres: \" + \n        df['genres'].apply(list_to_str) + \". Description: \" + \n        df['short_description']\n    )\n\n    num_cols = [\n        'price', 'dlc_count', 'metacritic_score', 'positive', 'negative', \n        'average_playtime_forever', 'median_playtime_forever', 'num_reviews_total'\n    ]\n    \n    for col in num_cols:\n\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        df[col] = df[col].fillna(0)\n\n        df[col] = df[col].clip(lower=0)\n\n    log_cols = ['positive', 'negative', 'num_reviews_total', 'average_playtime_forever']\n    for col in log_cols:\n        df[col] = np.log1p(df[col])\n\n    scaler = StandardScaler()\n    num_features = scaler.fit_transform(df[num_cols])\n\n    os_features = df[['windows', 'mac', 'linux']].fillna(False).astype(int).values\n\n    print(\"Запуск SentenceTransformer (может занять время)...\")\n    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n    text_embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True, batch_size=64)\n\n    final_matrix = np.hstack([text_embeddings, num_features, os_features])\n\n    game_map = {appid: vec for appid, vec in zip(df['appid'], final_matrix)}\n    \n    return game_map\n\ngame_vectors = preprocess_metadata(games_metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T02:11:33.306836Z","iopub.execute_input":"2025-12-21T02:11:33.307697Z","iopub.status.idle":"2025-12-21T02:15:37.166164Z","shell.execute_reply.started":"2025-12-21T02:11:33.307665Z","shell.execute_reply":"2025-12-21T02:15:37.165489Z"}},"outputs":[{"name":"stderr","text":"2025-12-21 02:11:45.657364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766283105.839846      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766283105.891412      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766283106.318741      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766283106.318775      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766283106.318778      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766283106.318780      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Генерация текста...\nЗапуск SentenceTransformer (может занять время)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"765c766b93a1471dae10cce0d672a78e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2997f3845e7459d877de8e2b96f1f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f70acca13e49eaaabced27614c83eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54167d4cd21d46d48ecfe1d4658f8de4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978d65107dee476c9cdd12c0a2418cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d413863a0084a91b149db8d7cd1ff23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a4c6bca58741078755e8106cb8c633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2e2c61e15e468d9a9617e6106b7479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a729c4e81c946c5949f575a4f791c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b406ad77b644db9833195674829d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1401 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d969de428b4859af456e50bcd4d219"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Dataset Formation and Negative Sampling Logic\n\nThis code block is responsible for preparing the data for neural network training. The main task here is to transform flat purchase lists into training examples that force the model to understand user preferences.\n\n#### Key Components:\n\n1. **User History (Context):**\n   * A game history is identified for each user. \n   * The `MAX_HISTORY = 10` parameter limits the number of previous games considered. If there are fewer games, **Padding** (filling with zeros) is applied so that the tensors have the same size for fast GPU processing.\n\n2. **Negative Sampling (Training Logic):**\n   * The model is trained using a comparison method. We provide it not only with the game the user bought (**Positive Target**) but also with a random game they did not buy (**Negative Target**).\n   * This forces the neural network to \"push\" the vectors: bringing the user profile closer to purchased games and moving it away from random ones.\n\n\n\n#### __getitem__ Method Mechanics:\n\n| Component | Formation Logic |\n| :--- | :--- |\n| **History Vecs** | Vectors of the user's latest games from `game_vectors`. If there are too few, they are supplemented with zero vectors. |\n| **Target Vec** | The vector of a randomly selected game from the user's real history (what we are trying to predict). |\n| **Neg Vec** | The vector of a game randomly selected from the entire `all_appids` catalog, excluding those the user has already bought. |","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport random\n\nuser_histories = train_interactions.groupby('playerid')['appid'].apply(list).to_dict()\nall_appids = list(game_vectors.keys())\n\nMAX_HISTORY = 10 \nNEG_SAMPLES = 2 \n\nclass SteamRecDataset(Dataset):\n    def __init__(self, user_histories, game_vectors, all_appids, is_train=True):\n        self.user_ids = list(user_histories.keys())\n        self.user_histories = user_histories\n        self.game_vectors = game_vectors\n        self.all_appids = set(all_appids)\n        self.is_train = is_train\n\n        self.emb_dim = len(next(iter(game_vectors.values())))\n\n    def __len__(self):\n        return len(self.user_ids)\n\n    def __getitem__(self, idx):\n        uid = self.user_ids[idx]\n        history = self.user_histories[uid]\n        \n        if len(history) < 2 and self.is_train:\n\n            target_game = history[0]\n            prev_games = history\n        else:\n\n            target_game = random.choice(history)\n            prev_games = [g for g in history if g != target_game]\n\n        prev_games = prev_games[-MAX_HISTORY:]\n\n        target_vec = torch.tensor(self.game_vectors[target_game], dtype=torch.float32)\n\n        history_vecs = [self.game_vectors[g] for g in prev_games]\n        while len(history_vecs) < MAX_HISTORY:\n            history_vecs.append(np.zeros(self.emb_dim))\n        history_vecs = torch.tensor(np.array(history_vecs), dtype=torch.float32)\n\n        if self.is_train:\n\n            neg_game = random.choice(list(self.all_appids - set(history)))\n            neg_vec = torch.tensor(self.game_vectors[neg_game], dtype=torch.float32)\n\n            return history_vecs, target_vec, neg_vec\n        else:\n\n            return history_vecs, target_vec\n\ntrain_dataset = SteamRecDataset(user_histories, game_vectors, all_appids)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T19:47:29.384698Z","iopub.execute_input":"2025-12-20T19:47:29.385517Z","iopub.status.idle":"2025-12-20T19:47:31.237815Z","shell.execute_reply.started":"2025-12-20T19:47:29.385486Z","shell.execute_reply":"2025-12-20T19:47:31.237110Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Model Architecture: SteamTransformer\n\nThis model is a hybrid neural network based on the **Transformer** architecture, adapted for recommendation tasks. It analyzes the sequence of a user's past games and estimates the probability of interaction with a new target game.\n\n#### Key Architectural Blocks:\n\n1. **Input Projection:**\n   * Input vectors (extracted from metadata and SBERT) have a high dimensionality. The `nn.Linear(input_dim, 256)` layer compresses them into a compact feature space (hidden dimension), which is more efficient for the transformer to process.\n\n2. **Transformer Encoder (Context Extraction):**\n   * **Self-Attention:** The attention mechanism allows the network to understand which games from the user's history are most relevant to the current moment. For example, if there are 10 games in the history, but the last 2 are shooters, the attention will focus on them when evaluating a new game.\n   * **Multi-head Attention (4 heads):** Allows the model to simultaneously track different types of dependencies (e.g., genre similarity and price similarity).\n   \n\n3. **Feature Aggregation:**\n   * After the transformer, we obtain a set of vectors (one for each game in the history). Using `torch.mean(..., dim=1)`, we combine them into a single **\"User Interest Vector\"**. This is an averaged representation of the player's current preferences.\n\n4. **Classifier Head:**\n   * The model concatenates the user vector and the target game vector (`torch.cat`).\n   * **Fully Connected Layers:** A Multi-Layer Perceptron (MLP) analyzes this combination.\n   * **Dropout (0.2):** Prevents overfitting by forcing the network not to rely on specific neurons.\n   * **Sigmoid:** The final layer outputs the probability of purchase.\n   \n\n#### Technical Specifications:\n* **Hidden Dimension:** 256\n* **Feedforward Dimension:** 512 (inside the transformer)\n* **Dropout:** 0.2\n* **Output:** Probability of purchase/interaction.\n\n#### Advantage over Base Models:\nUnlike simple fully connected networks, `SteamTransformer` accounts for the **relationships between games within the history**. It understands not just \"what the user bought,\" but \"how their purchases relate to one another,\" which is critical for the complex video game market.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass SteamTransformer(nn.Module):\n    def __init__(self, input_dim, nhead=4, num_layers=2):\n        super().__init__()\n\n        self.input_proj = nn.Linear(input_dim, 256)\n=\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=256, nhead=nhead, dim_feedforward=512, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.fc = nn.Sequential(\n            nn.Linear(256 + 256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, history, target):\n        hist_emb = self.input_proj(history)  \n        target_emb = self.input_proj(target) \n \n        user_features = self.transformer(hist_emb)\n\n        user_combined = torch.mean(user_features, dim=1) \n\n        combined = torch.cat([user_combined, target_emb], dim=-1)\n        return self.fc(combined)\n\ninput_size = len(next(iter(game_vectors.values())))\nmodel = SteamTransformer(input_dim=input_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T19:48:01.096057Z","iopub.execute_input":"2025-12-20T19:48:01.096382Z","iopub.status.idle":"2025-12-20T19:48:01.120924Z","shell.execute_reply.started":"2025-12-20T19:48:01.096355Z","shell.execute_reply":"2025-12-20T19:48:01.120006Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Validation of the model: HitRate@K and NDCG@K\n\nThe block is designed to evaluate the accuracy of recommendations based on a training sample.\n","metadata":{}},{"cell_type":"code","source":"def evaluate_metrics(model, game_vectors, user_histories, test_interactions, k=10):\n    model.eval()\n    hr, ndcg = 0, 0\n    users = test_interactions['playerid'].unique()\n\n    device = next(model.parameters()).device\n    \n    all_appids = list(game_vectors.keys())\n    emb_dim = len(next(iter(game_vectors.values())))\n\n    with torch.no_grad():\n        for uid in users:\n            history = user_histories.get(uid, [])[-MAX_HISTORY:]\n            if not history: continue\n\n            h_vecs = [game_vectors[g] for g in history]\n            while len(h_vecs) < MAX_HISTORY:\n                h_vecs.append(np.zeros(emb_dim))\n\n            h_tensor = torch.tensor(np.array(h_vecs), dtype=torch.float32).unsqueeze(0).to(device)\n\n            pos_game = test_interactions[test_interactions['playerid'] == uid]['appid'].iloc[0]\n\n            neg_candidates = list(set(all_appids) - set(user_histories.get(uid, [])))\n            neg_games = random.sample(neg_candidates, min(99, len(neg_candidates)))\n            \n            candidate_games = [pos_game] + neg_games\n            c_vecs = torch.tensor(np.array([game_vectors[g] for g in candidate_games]), dtype=torch.float32).to(device)\n\n            num_candidates = len(candidate_games)\n            h_tensor_expanded = h_tensor.expand(num_candidates, -1, -1)\n\n            scores = model(h_tensor_expanded, c_vecs).squeeze().cpu().numpy()\n\n            rank_indices = np.argsort(scores)[::-1]\n            try:\n                rank = np.where(rank_indices == 0)[0][0] + 1 \n                if rank <= k:\n                    hr += 1\n                    ndcg += 1 / math.log2(rank + 1)\n            except IndexError:\n\n                continue\n\n    final_hr = hr / len(users)\n    final_ndcg = ndcg / len(users)\n    return final_hr, final_ndcg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:11:15.098408Z","iopub.execute_input":"2025-12-20T20:11:15.099248Z","iopub.status.idle":"2025-12-20T20:11:15.108558Z","shell.execute_reply.started":"2025-12-20T20:11:15.099214Z","shell.execute_reply":"2025-12-20T20:11:15.107866Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Model Training Process\n\nThis block implements the neural network training loop using Binary Cross-Entropy (BCE).\n\n#### Key Parameters:\n* **Optimizer**: `Adam` (lr=0.0005) with L2 regularization (`weight_decay`).\n* **Loss Function**: `BCELoss`.\n* **Configuration**: 5 training epochs.\n\n\n\n1. **Dual Pass**: For each batch, predictions are calculated separately for \"positive\" games and \"negative\" candidates.\n2. **Balancing**: The average loss value `(pos_loss + neg_loss) / 2` allows for uniform adjustment of the model weights.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm.auto import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\ncriterion = nn.BCELoss()\n\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    for history_vecs, pos_vecs, neg_vecs in pbar:\n        history_vecs = history_vecs.to(device)\n        pos_vecs = pos_vecs.to(device)\n        neg_vecs = neg_vecs.to(device)\n        \n        optimizer.zero_grad()\n  \n        pos_preds = model(history_vecs, pos_vecs)\n        pos_loss = criterion(pos_preds, torch.ones_like(pos_preds))\n\n        neg_preds = model(history_vecs, neg_vecs)\n        neg_loss = criterion(neg_preds, torch.zeros_like(neg_preds))\n        \n        # Общая ошибка\n        loss = (pos_loss + neg_loss) / 2\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n    print(f\"Запуск валидации...\")\n    hr, ndcg = evaluate_metrics(model, game_vectors, user_histories, test_interactions, k=10)\n    print(f\"Epoch {epoch+1} finished. Loss: {total_loss/len(train_loader):.4f} | HR@10: {hr:.4f} | NDCG@10: {ndcg:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:11:18.080564Z","iopub.execute_input":"2025-12-20T20:11:18.081260Z","iopub.status.idle":"2025-12-20T21:17:24.000934Z","shell.execute_reply.started":"2025-12-20T20:11:18.081231Z","shell.execute_reply":"2025-12-20T21:17:23.999736Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/5:   0%|          | 0/688 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74f156ce0d9d4ec1aedfd9626bfd7456"}},"metadata":{}},{"name":"stdout","text":"Запуск валидации...\nEpoch 1 finished. Loss: 0.1796 | HR@10: 0.9465 | NDCG@10: 0.7619\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/5:   0%|          | 0/688 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01900133c9c24b93bf0f82c72e26ccdd"}},"metadata":{}},{"name":"stdout","text":"Запуск валидации...\nEpoch 2 finished. Loss: 0.1715 | HR@10: 0.9486 | NDCG@10: 0.7730\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/5:   0%|          | 0/688 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46363ba5713d4419a32727a9e4b3be86"}},"metadata":{}},{"name":"stdout","text":"Запуск валидации...\nEpoch 3 finished. Loss: 0.1703 | HR@10: 0.9478 | NDCG@10: 0.7701\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/5:   0%|          | 0/688 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e24c0d43a8984cb5bed72000cb934253"}},"metadata":{}},{"name":"stdout","text":"Запуск валидации...\nEpoch 4 finished. Loss: 0.1704 | HR@10: 0.9517 | NDCG@10: 0.7821\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/5:   0%|          | 0/688 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6605a050c4a346b0a3a3e2bbc7085ae6"}},"metadata":{}},{"name":"stdout","text":"Запуск валидации...\nEpoch 5 finished. Loss: 0.1684 | HR@10: 0.9501 | NDCG@10: 0.7742\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### The main validation function","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nfrom tqdm import tqdm\n\n\ndef calculate_metrics(test_df, recommender_model, k=10):\n    \"\"\"\n    Calculates HitRate@K, Recall@K, NDCG@K\n    \"\"\"\n    known_users = set(recommender_model.user_map.keys())\n    test_df_filtered = test_df[test_df['playerid'].isin(known_users)].copy()\n\n    ground_truth = test_df_filtered.groupby('playerid')['appid'].apply(list).to_dict()\n\n    hits = 0\n    total_recall = 0\n    total_ndcg = 0\n    n_users = len(ground_truth)\n\n    if n_users == 0:\n        return {\"Error\": \"No overlapping users in test set\"}\n\n    for user, actual_items in tqdm(ground_truth.items()):\n        recs = recommender_model.recommend(user, top_k=k)\n\n        hit = any(item in actual_items for item in recs)\n        if hit:\n            hits += 1\n\n        intersect = set(recs).intersection(set(actual_items))\n        recall = len(intersect) / len(actual_items) if len(actual_items) > 0 else 0\n        total_recall += recall\n\n        dcg = 0\n        idcg = 0\n\n        for i, item in enumerate(recs):\n            if item in actual_items:\n                dcg += 1 / math.log2((i + 1) + 1)\n\n        num_relevant_in_top_k = min(len(actual_items), k)\n        for i in range(num_relevant_in_top_k):\n            idcg += 1 / math.log2((i + 1) + 1)\n\n        ndcg = dcg / idcg if idcg > 0 else 0\n        total_ndcg += ndcg\n\n    return {\n        f\"HitRate@{k}\": hits / n_users,\n        f\"Recall@{k}\": total_recall / n_users,\n        f\"NDCG@{k}\": total_ndcg / n_users\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:17:36.469935Z","iopub.execute_input":"2025-12-20T21:17:36.470684Z","iopub.status.idle":"2025-12-20T21:17:36.478399Z","shell.execute_reply.started":"2025-12-20T21:17:36.470641Z","shell.execute_reply":"2025-12-20T21:17:36.477511Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### Wrapper for the model","metadata":{}},{"cell_type":"code","source":"class SteamRecommender:\n    def __init__(self, model, user_histories, game_vectors, all_appids, device):\n        self.model = model\n        self.user_histories = user_histories\n        self.game_vectors = game_vectors\n        self.all_appids = all_appids\n        self.device = device\n\n        self.user_map = user_histories \n\n        self.candidate_pool = all_appids \n\n    def recommend(self, user_id, top_k=10):\n        self.model.eval()\n\n        history = self.user_histories.get(user_id, [])[-MAX_HISTORY:]\n        if not history:\n            return []\n\n        emb_dim = len(next(iter(self.game_vectors.values())))\n        h_vecs = [self.game_vectors.get(g, np.zeros(emb_dim)) for g in history]\n        while len(h_vecs) < MAX_HISTORY:\n            h_vecs.append(np.zeros(emb_dim))\n        \n        h_tensor = torch.tensor(np.array(h_vecs), dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        bought_games = set(history)\n        candidates = [g for g in self.candidate_pool if g not in bought_games]\n\n        c_vecs = torch.tensor(np.array([self.game_vectors[g] for g in candidates]), dtype=torch.float32).to(self.device)\n        \n        with torch.no_grad():\n\n            h_expanded = h_tensor.expand(len(candidates), -1, -1)\n            scores = self.model(h_expanded, c_vecs).squeeze().cpu().numpy()\n\n        top_indices = np.argsort(scores)[::-1][:top_k]\n        return [candidates[i] for i in top_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:17:38.596811Z","iopub.execute_input":"2025-12-20T21:17:38.597154Z","iopub.status.idle":"2025-12-20T21:17:38.605428Z","shell.execute_reply.started":"2025-12-20T21:17:38.597115Z","shell.execute_reply":"2025-12-20T21:17:38.604559Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"recommender = SteamRecommender(\n    model=model, \n    user_histories=user_histories, \n    game_vectors=game_vectors, \n    all_appids=all_appids,\n    device=device\n)\n\nmetrics = calculate_metrics(test_interactions[:1000], recommender, k=10)\nprint(f\"Final Test Metrics: {metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:21:19.676782Z","iopub.execute_input":"2025-12-20T21:21:19.677627Z","iopub.status.idle":"2025-12-20T21:44:12.226461Z","shell.execute_reply.started":"2025-12-20T21:21:19.677592Z","shell.execute_reply":"2025-12-20T21:44:12.225643Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [22:52<00:00,  1.37s/it]","output_type":"stream"},{"name":"stdout","text":"Final Test Metrics: {'HitRate@10': 0.018, 'Recall@10': 0.018, 'NDCG@10': 0.006429227523346531}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Why did the approach fail?\n\nDespite using advanced embeddings, this approach encountered several critical barriers:\n\n#### 1. Semantic Noise and \"Marketing\" Text\nModels like **SBERT** are excellent at capturing textual meaning, but Steam descriptions are full of artistic epithets.\n* **Problem:** Two games might have similar descriptions (\"epic adventure\", \"open world\") but radically different gameplay (platformer vs. strategy). The model saw textual similarity but failed to recognize the difference in the actual gaming experience.\n\n\n\n#### 2. Ignoring Popularity (Popular != Similar)\n The content-based approach looks for *similarity*, but purchases on Steam are often driven by trends and popularity.\n* **Problem:** KNN might recommend an \"ideally similar\" game that has 0 reviews and 0 players, instead of a hit that the user would actually be likely to buy.\n\n#### 3. Lack of Collaborative Signal\nThis version relied solely on the properties of the game itself, completely ignoring the behavior of other people.\n* **Problem:** The most powerful signal (\"People who bought A also bought B\") was not taken into account. Content-based KNN did not know that games could be related in meaning even if their descriptions were not similar.\n\n\n\n#### 4. \"Curse of Dimensionality\"\nWhen combining 384 SBERT dimensions with other features, the vectors became too \"heavy.\"\n* **Problem:** In high-dimensional spaces, cosine similarity begins to degrade — all games become \"equally distant\" from each other to the model, and search accuracy drops.","metadata":{}},{"cell_type":"markdown","source":"## Hybrid Pipeline: Two-Tower Transformer with BPR Loss\n\nThis code block implements a modern recommendation system architecture that combines content-based features (SBERT + Meta) with collaborative learning.\n\n#### Key Pipeline Stages:\n\n1. **Feature Compression (PCA):**\n    * The initial `game_vectors` (from SBERT) have high dimensionality.\n    * **PCA** reduces them to the 64 most informative components, which minimizes noise and accelerates neural network computations.\n\n\n\n2. **Two-Tower Architecture:**\n    * **Item Tower:** Combines static features (Content) and trainable embeddings (ID). This allows the model to recognize a game as both an \"RPG with a Fantasy tag\" and a specific object with unique user behavior.\n    * **User Tower (Transformer):** Processes a sequence of game vectors from the user's history. The Transformer identifies complex dependencies between purchases to form a final user interest vector.\n\n\n\n3. **Training via BPR (Bayesian Personalized Ranking):**\n    * Instead of \"bought/not bought\" classification, the model learns **ranking**.\n    * **Negative Sampling:** For every actual purchase (Positive), a random game the user did not purchase (Negative) is selected.\n    * **Goal:** To make the dot product of the user vector and the \"good\" game higher than with the \"bad\" game.\n\n\n\n4. **Inference Optimization:**\n    * After training, vectors for all games (`all_item_vectors`) are pre-calculated.\n    * This transforms recommendation generation from a heavy neural network pass into instantaneous matrix multiplication (Dot Product).\n\n| Parameter | Value | Description |\n| :--- | :--- | :--- |\n| **EMBED_DIM** | 64 | Dimensionality of the model's internal space. |\n| **MAX_HISTORY** | 10 | Depth of user history analysis. |\n| **Loss** | BPR | Ranking order optimization. |\n| **Device** | CUDA/CPU | Automatic GPU switching for acceleration. |","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.decomposition import PCA\nfrom tqdm.auto import tqdm\n\nprint(\"Подготовка признаков и маппинга...\")\nunique_apps = sorted(games_metadata['appid'].unique())\napp2idx = {appid: i + 1 for i, appid in enumerate(unique_apps)}\nidx2app = {i: appid for appid, i in app2idx.items()}\n\npop_counts = train_interactions['appid'].value_counts()\npop_dict = pop_counts.to_dict()\n\nraw_feat_matrix = np.array([game_vectors[aid] for aid in unique_apps])\npca = PCA(n_components=64) \ncompressed_feats = pca.fit_transform(raw_feat_matrix)\n\ngame_feats_matrix = np.zeros((len(unique_apps) + 1, 64))\ngame_feats_matrix[1:] = compressed_feats\ngame_feats_tensor = torch.FloatTensor(game_feats_matrix)\n\nMAX_HISTORY = 10\nEMBED_DIM = 64\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass SteamDataset(Dataset):\n    def __init__(self, interactions, app2idx, max_history=10):\n        self.user_histories = interactions.groupby('playerid')['appid'].apply(list).to_dict()\n        self.user_ids = list(self.user_histories.keys())\n        self.app2idx = app2idx\n        self.all_apps = list(app2idx.keys())\n        self.max_history = max_history\n\n    def __len__(self):\n        return len(self.user_ids)\n\n    def __getitem__(self, idx):\n        uid = self.user_ids[idx]\n        full_history = self.user_histories[uid]\n\n        t_idx = np.random.randint(0, len(full_history))\n        target_pos = full_history[t_idx]\n\n        context = [g for i, g in enumerate(full_history) if i != t_idx][-self.max_history:]\n\n        user_bought = set(full_history)\n        while True:\n            target_neg = np.random.choice(self.all_apps)\n            if target_neg not in user_bought:\n                break\n\n        hist_ids = [self.app2idx.get(aid, 0) for aid in context]\n        while len(hist_ids) < self.max_history:\n            hist_ids.append(0) \n            \n        return (\n            torch.LongTensor(hist_ids),\n            torch.LongTensor([self.app2idx[target_pos]]),\n            torch.LongTensor([self.app2idx[target_neg]])\n        )\n\nclass SteamTwoTower(nn.Module):\n    def __init__(self, num_apps, game_feats_tensor, embed_dim=64):\n        super().__init__()\n        self.game_content = nn.Embedding.from_pretrained(game_feats_tensor, freeze=True)\n        self.id_emb = nn.Embedding(num_apps + 1, embed_dim)\n\n        self.merge = nn.Linear(embed_dim + 64, embed_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n\n    def get_item_vec(self, ids):\n        comb = torch.cat([self.id_emb(ids), self.game_content(ids)], dim=-1)\n        return self.merge(comb)\n\n    def forward(self, hist_ids, pos_ids, neg_ids):\n        hist_vecs = self.get_item_vec(hist_ids)\n        user_out = self.transformer(hist_vecs)\n        user_vec = torch.mean(user_out, dim=1) \n\n        pos_vec = self.get_item_vec(pos_ids).squeeze(1)\n        neg_vec = self.get_item_vec(neg_ids).squeeze(1)\n\n        return user_vec, pos_vec, neg_vec\n\nmodel = SteamTwoTower(len(app2idx), game_feats_tensor, EMBED_DIM).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ntrain_dataset = SteamDataset(train_interactions, app2idx, MAX_HISTORY)\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n\nprint(\"Начало обучения...\")\nmodel.train()\nfor epoch in range(10):\n    total_loss = 0\n    for h_ids, p_ids, n_ids in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        h_ids, p_ids, n_ids = h_ids.to(DEVICE), p_ids.to(DEVICE), n_ids.to(DEVICE)\n        \n        optimizer.zero_grad()\n        user_v, pos_v, neg_v = model(h_ids, p_ids, n_ids)\n\n        pos_scores = (user_v * pos_v).sum(dim=-1)\n        neg_scores = (user_v * neg_v).sum(dim=-1)\n\n        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-9).mean()\n        \n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n\nuser_histories_final = train_interactions.groupby('playerid')['appid'].apply(list).to_dict()\n\nmodel.eval()\nwith torch.no_grad():\n    all_item_ids = torch.arange(len(app2idx) + 1).to(DEVICE)\n    all_item_vectors = model.get_item_vec(all_item_ids).cpu().numpy()\n\nprint(\"Пайплайн готов к расчету метрик.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:52:34.015598Z","iopub.execute_input":"2025-12-20T21:52:34.015997Z","iopub.status.idle":"2025-12-20T22:56:18.320059Z","shell.execute_reply.started":"2025-12-20T21:52:34.015965Z","shell.execute_reply":"2025-12-20T22:56:18.319131Z"}},"outputs":[{"name":"stdout","text":"Подготовка признаков и маппинга...\nНачало обучения...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17cff39505124787b6dd25abb9013b26"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Loss: 0.1399\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f0fe1d87d344129574ce5f8a2d1dd7"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Loss: 0.1010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9fd5112f5b4c5cbcbebe96a5d66a35"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 Loss: 0.0993\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb666a425eb41f88fa9a8f747d62a2f"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 Loss: 0.0969\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee25a474b3834ad6970020643aaae2a3"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 Loss: 0.0903\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb3a0c3889549ea9cb3133be6923785"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 Loss: 0.0955\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b8fb5ed03d74c869a1bb558f6e0dcda"}},"metadata":{}},{"name":"stdout","text":"Epoch 7 Loss: 0.0884\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c837ab2c9e564f7484e73705028fd1b6"}},"metadata":{}},{"name":"stdout","text":"Epoch 8 Loss: 0.0889\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1999491dafc3470e93091b6c1d461dc1"}},"metadata":{}},{"name":"stdout","text":"Epoch 9 Loss: 0.0923\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a0d9de5303478eb6924808f22a11cf"}},"metadata":{}},{"name":"stdout","text":"Epoch 10 Loss: 0.0830\nПайплайн готов к расчету метрик.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"class SteamInferenceModel:\n    def __init__(self, model, app2idx, user_histories, item_vectors, device):\n        self.model = model\n        self.app2idx = app2idx\n        self.user_map = user_histories  \n        self.item_vectors = item_vectors \n        self.device = device\n        self.all_appids = sorted(app2idx.keys())\n        self.idx2app = {i: aid for aid, i in app2idx.items()}\n\n    def recommend(self, user_id, top_k=10):\n        self.model.eval()\n\n        history = self.user_map.get(user_id, [])[-MAX_HISTORY:]\n        if not history:\n            return []\n\n        hist_ids = [self.app2idx.get(aid, 0) for aid in history]\n        while len(hist_ids) < MAX_HISTORY:\n            hist_ids.append(0)\n        \n        h_tensor = torch.LongTensor(hist_ids).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n\n            hist_vecs = self.model.get_item_vec(h_tensor)\n            user_vec = torch.mean(self.model.transformer(hist_vecs), dim=1).cpu().numpy()\n\n        scores = np.dot(self.item_vectors, user_vec.T).flatten()\n\n        bought_indices = [self.app2idx.get(aid, 0) for aid in history]\n        scores[bought_indices] = -1e9\n        scores[0] = -1e9\n\n        top_indices = np.argsort(scores)[::-1][:top_k]\n        return [self.idx2app[i] for i in top_indices if i in self.idx2app]\n\nrecommender = SteamInferenceModel(\n    model=model, \n    app2idx=app2idx, \n    user_histories=user_histories_final, \n    item_vectors=all_item_vectors, \n    device=DEVICE\n)\n\nfinal_results = calculate_metrics(test_interactions, recommender, k=10)\nprint(\"\\n=== FINAL TEST RESULTS ===\")\nfor m, v in final_results.items():\n    print(f\"{m}: {v:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T23:01:25.122614Z","iopub.execute_input":"2025-12-20T23:01:25.123546Z","iopub.status.idle":"2025-12-20T23:04:41.120929Z","shell.execute_reply.started":"2025-12-20T23:01:25.123504Z","shell.execute_reply":"2025-12-20T23:04:41.119934Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/44021 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa1cdc1ac9846b39994b43ab6f09e7c"}},"metadata":{}},{"name":"stdout","text":"\n=== FINAL TEST RESULTS ===\nHitRate@10: 0.0483\nRecall@10: 0.0483\nNDCG@10: 0.0157\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"### Analysis and Conclusions\n\nDespite the use of a complex **Two-Tower Transformer** architecture, the final metrics (**HitRate@10: 0.0483**) were lower than standard methods. This was caused by the following reasons:\n\n#### 1. Dominance of the \"Collaborative Signal\"\nIn Steam data, community behavior (social signal) is the most accurate predictor.\n* **KNN** directly exploits this signal through the principle: *\"People who bought this also bought that\"*.\n* The **Neural Network** attempts to learn these connections indirectly through embeddings. To \"catch up\" with KNN in terms of memorizing such dependencies, a significantly larger volume of data is required.\n\n\n\n#### 2. Sparsity and PCA Issues\nTo reduce the dimensionality of SBERT embeddings, **PCA** was used (compression to 64 components).\n* **Consequence:** Along with noise, subtle semantic differences might have been removed. If two games differed only by one specific tag, PCA could have \"averaged\" their vectors, making them indistinguishable to the model.","metadata":{}}]}