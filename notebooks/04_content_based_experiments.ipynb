{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5f0b2c",
   "metadata": {},
   "source": [
    "# 04 â€” Content-Based Recommender (Experiments)\n",
    "Content-based hybrids for Steam games using BM25-weighted metadata (genres/tags/categories/developers), dense features, and popularity blending. Includes feature-kNN, hybrid alphas, and heavier LightFM sweeps. Shared evaluation (HitRate/Recall/NDCG) matches teammate CF metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b75be7",
   "metadata": {},
   "source": [
    "## Approach\n",
    "- Interaction signal: purchases (`playerid`, `appid`).\n",
    "- Item representation: BM25 for text fields (`tags`/`categories`/`developers`), bucketed price/owners, base dense features; optional TruncatedSVD.\n",
    "- Models: popularity baseline; content+pop hybrids (alpha sweep); feature-kNN (precomputed neighbors); LightFM hybrid sweep (factors/epochs/loss).\n",
    "- Evaluation: HitRate/Recall/NDCG via `src.evaluation.evaluate_model`, excluding seen items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bcc248-94a2-4abe-a9e2-75624451acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/alyx/Documents/RS/Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535f12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import config\n",
    "from src.evaluation import build_ground_truth, evaluate_model\n",
    "from src.models.popularity import PopularityRecommender\n",
    "from src.models.content_based import ContentHybridRecommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7f7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment params\n",
    "SAMPLE_USERS = 2000\n",
    "MIN_INTERACTIONS = 10\n",
    "ALPHAS = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "KNN_NEIGHBORS = [50, 100, 150]\n",
    "VOCAB_TAGS = 4000\n",
    "VOCAB_CATEGORIES = 2000\n",
    "VOCAB_DEVELOPERS = 1000\n",
    "USE_SVD = True\n",
    "SVD_COMPONENTS = [128, 256]\n",
    "BM25_K1 = 1.6\n",
    "BM25_B = 0.75\n",
    "# LightFM heavy sweep (optional)\n",
    "LIGHTFM_FACTORS = [32, 64, 128]\n",
    "LIGHTFM_EPOCHS = [5, 10]\n",
    "LIGHTFM_LOSSES = [\"warp\", \"bpr\"]\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b0ea2",
   "metadata": {},
   "source": [
    "## Load processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d090ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52210/3239961831.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  item_features = pd.read_parquet(config.PROCESSED_DATA_DIR / \"item_features.parquet\").fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shapes: (9117646, 2) (44021, 2) (89618, 4239) (89618, 47)\n"
     ]
    }
   ],
   "source": [
    "USER_COL = config.USER_COL\n",
    "ITEM_COL = config.ITEM_COL\n",
    "\n",
    "train_df = pd.read_parquet(config.PROCESSED_DATA_DIR / \"train_interactions.parquet\")\n",
    "test_df = pd.read_parquet(config.PROCESSED_DATA_DIR / \"test_interactions.parquet\")\n",
    "item_features = pd.read_parquet(config.PROCESSED_DATA_DIR / \"item_features.parquet\").fillna(0)\n",
    "games_meta = pd.read_parquet(config.PROCESSED_DATA_DIR / \"games_metadata.parquet\")\n",
    "\n",
    "print(\"Raw shapes:\", train_df.shape, test_df.shape, item_features.shape, games_meta.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d2c0f",
   "metadata": {},
   "source": [
    "## Optional sampling for fast iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee73fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 2000 users -> train (454394, 2), test (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "user_counts = train_df[USER_COL].value_counts()\n",
    "eligible_users = user_counts[user_counts >= MIN_INTERACTIONS].index\n",
    "\n",
    "if SAMPLE_USERS:\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    sample_size = min(SAMPLE_USERS, len(eligible_users))\n",
    "    sampled_users = rng.choice(eligible_users, size=sample_size, replace=False)\n",
    "    train_df = train_df[train_df[USER_COL].isin(sampled_users)].copy()\n",
    "    test_df = test_df[test_df[USER_COL].isin(sampled_users)].copy()\n",
    "    print(f\"Sampled {sample_size} users -> train {train_df.shape}, test {test_df.shape}\")\n",
    "else:\n",
    "    train_df = train_df[train_df[USER_COL].isin(eligible_users)].copy()\n",
    "    test_df = test_df[test_df[USER_COL].isin(eligible_users)].copy()\n",
    "    print(f\"Using all eligible users -> train {train_df.shape}, test {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b7a4e",
   "metadata": {},
   "source": [
    "## Feature helpers (BM25 + dense + optional SVD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74306273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_block(series: pd.Series, prefix: str, max_features: int, k1: float = BM25_K1, b: float = BM25_B):\n",
    "    texts = series.fillna(\"\").astype(str).tolist()\n",
    "    vec = CountVectorizer(max_features=max_features)\n",
    "    X = vec.fit_transform(texts)\n",
    "    tf = X\n",
    "    dl = np.asarray(tf.sum(axis=1)).ravel()\n",
    "    avg_dl = dl.mean() + 1e-8\n",
    "    idf = np.log((tf.shape[0] - tf.astype(bool).sum(axis=0) + 0.5) / (tf.astype(bool).sum(axis=0) + 0.5)) + 1\n",
    "    idf = np.asarray(idf).ravel()\n",
    "    denom = tf + k1 * (1 - b + b * (dl / avg_dl))[:, None]\n",
    "    numer = tf.multiply(k1 + 1)\n",
    "    bm25 = numer.multiply(1 / denom)\n",
    "    bm25 = bm25.multiply(idf)\n",
    "    names = [f\"{prefix}{t}\" for t in vec.get_feature_names_out()]\n",
    "    return bm25.tocsr(), names\n",
    "\n",
    "\n",
    "def build_feature_matrix(base_feats: pd.DataFrame, meta: pd.DataFrame, use_svd: bool, svd_components: Optional[int]):\n",
    "    items = base_feats[ITEM_COL].astype(int).tolist()\n",
    "    meta_aligned = meta.set_index(ITEM_COL).reindex(base_feats[ITEM_COL]).reset_index()\n",
    "\n",
    "    blocks = []\n",
    "\n",
    "    base_dense = csr_matrix(base_feats.drop(columns=[ITEM_COL]).to_numpy(dtype=np.float32))\n",
    "    blocks.append(base_dense)\n",
    "\n",
    "    # Price bins\n",
    "    price_col = config.PRICE_COL\n",
    "    if price_col in meta_aligned.columns:\n",
    "        prices = pd.to_numeric(meta_aligned[price_col], errors=\"coerce\").fillna(0)\n",
    "        bins = [0, 1, 5, 10, 20, 50, 100, np.inf]\n",
    "        labels = [f\"price_bin_{i}\" for i in range(len(bins)-1)]\n",
    "        price_bins = pd.get_dummies(pd.cut(prices, bins=bins, labels=labels, include_lowest=True))\n",
    "    else:\n",
    "        price_bins = pd.DataFrame(index=meta_aligned.index)\n",
    "\n",
    "    # Owners bins\n",
    "    if \"estimated_owners\" in meta_aligned.columns:\n",
    "        owners_raw = meta_aligned[\"estimated_owners\"].fillna(\"\")\n",
    "        def parse_owner(val):\n",
    "            if isinstance(val, str) and \"-\" in val:\n",
    "                try:\n",
    "                    low = val.split(\"-\")[0].replace(\",\", \"\").strip()\n",
    "                    return float(low)\n",
    "                except Exception:\n",
    "                    return np.nan\n",
    "            try:\n",
    "                return float(val)\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        owners_num = owners_raw.apply(parse_owner)\n",
    "        bins = [0, 1e3, 1e4, 1e5, 1e6, 1e7, np.inf]\n",
    "        labels = [f\"owners_bin_{i}\" for i in range(len(bins)-1)]\n",
    "        owner_bins = pd.get_dummies(pd.cut(owners_num, bins=bins, labels=labels, include_lowest=True))\n",
    "    else:\n",
    "        owner_bins = pd.DataFrame(index=meta_aligned.index)\n",
    "\n",
    "    extra_dense = pd.concat([price_bins, owner_bins], axis=1).fillna(0)\n",
    "    blocks.append(csr_matrix(extra_dense.to_numpy(dtype=np.float32)))\n",
    "\n",
    "    # BM25 text blocks\n",
    "    if \"categories\" in meta_aligned.columns:\n",
    "        mat, _ = bm25_block(meta_aligned[\"categories\"], prefix=\"cat::\", max_features=VOCAB_CATEGORIES)\n",
    "        blocks.append(mat)\n",
    "    if \"developers\" in meta_aligned.columns:\n",
    "        mat, _ = bm25_block(meta_aligned[\"developers\"], prefix=\"dev::\", max_features=VOCAB_DEVELOPERS)\n",
    "        blocks.append(mat)\n",
    "    if \"tags\" in meta_aligned.columns:\n",
    "        mat, _ = bm25_block(meta_aligned[\"tags\"], prefix=\"tag::\", max_features=VOCAB_TAGS)\n",
    "        blocks.append(mat)\n",
    "\n",
    "    matrix = hstack(blocks).tocsr()\n",
    "\n",
    "    if use_svd and svd_components:\n",
    "        svd = TruncatedSVD(n_components=svd_components, random_state=RANDOM_STATE)\n",
    "        matrix = svd.fit_transform(matrix)\n",
    "        matrix = normalize(matrix)\n",
    "        matrix = csr_matrix(matrix)\n",
    "\n",
    "    matrix = normalize(matrix, norm=\"l2\", axis=1)\n",
    "    item_to_idx = {iid: i for i, iid in enumerate(items)}\n",
    "    return items, item_to_idx, matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717e81e",
   "metadata": {},
   "source": [
    "## Prepare features (with SVD options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a3589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_split = set(train_df[ITEM_COL]) | set(test_df[ITEM_COL])\n",
    "base_feats = item_features[item_features[ITEM_COL].isin(items_in_split)].copy().reset_index(drop=True)\n",
    "meta_filtered = games_meta[games_meta[ITEM_COL].isin(items_in_split)].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055da4a2",
   "metadata": {},
   "source": [
    "## Evaluation setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49914aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users for eval: 2000\n"
     ]
    }
   ],
   "source": [
    "ground_truth = build_ground_truth(test_df, user_col=USER_COL, item_col=ITEM_COL)\n",
    "users_eval = list(ground_truth.keys())\n",
    "known_items_map = train_df.groupby(USER_COL)[ITEM_COL].apply(list).to_dict()\n",
    "print(f\"Users for eval: {len(users_eval)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c458b9f",
   "metadata": {},
   "source": [
    "## Feature-kNN helper (precomputed neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88182ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedFeatureKNN:\n",
    "    def __init__(self, item_matrix: csr_matrix, item_ids: List[int], item_to_idx: Dict[int, int], max_neighbors: int = 200):\n",
    "        self.item_matrix = item_matrix\n",
    "        self.item_ids = item_ids\n",
    "        self.item_to_idx = item_to_idx\n",
    "        self.max_neighbors = min(max_neighbors, item_matrix.shape[0]-1)\n",
    "        knn = NearestNeighbors(metric=\"cosine\", n_neighbors=self.max_neighbors)\n",
    "        knn.fit(item_matrix)\n",
    "        distances, neighbors = knn.kneighbors(item_matrix, n_neighbors=self.max_neighbors)\n",
    "        self.neighbors = neighbors\n",
    "        self.sims = 1 - distances\n",
    "        self.default_n_neighbors = self.max_neighbors\n",
    "\n",
    "    def recommend(self, user_id: int, known_items: List[int], k: int) -> List[int]:\n",
    "        if not known_items:\n",
    "            return []\n",
    "        known_idx = [self.item_to_idx[i] for i in known_items if i in self.item_to_idx]\n",
    "        if not known_idx:\n",
    "            return []\n",
    "        scores = np.zeros(self.item_matrix.shape[0], dtype=np.float32)\n",
    "        n_use = self.default_n_neighbors\n",
    "        for idx in known_idx:\n",
    "            neigh = self.neighbors[idx, :n_use]\n",
    "            sim = self.sims[idx, :n_use]\n",
    "            scores[neigh] += sim\n",
    "        for idx in known_idx:\n",
    "            scores[idx] = -np.inf\n",
    "        top_idx = np.argpartition(scores, -k)[-k:]\n",
    "        top_idx = top_idx[np.argsort(scores[top_idx])[::-1]]\n",
    "        return [self.item_ids[i] for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc5531",
   "metadata": {},
   "source": [
    "## Sweeps (hybrids, kNN, SVD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398079b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 5) (1261904693.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "svd_grid = SVD_COMPONENTS if USE_SVD else [None]\n",
    "\n",
    "for svd_comp in tqdm(svd_grid, desc=\"SVD settings\"):\n",
    "    print(f\"=== Feature store (SVD={svd_comp}) ===\")\n",
    "    item_ids, item_to_idx, item_matrix = build_feature_matrix(\n",
    "        base_feats=base_feats,\n",
    "        meta=meta_filtered,\n",
    "        use_svd=USE_SVD,\n",
    "        svd_components=svd_comp,\n",
    "    )\n",
    "    pop_counts = train_df[ITEM_COL].value_counts()\n",
    "    pop_ranking = pop_counts.index.tolist()\n",
    "    pop_scores = np.zeros(len(item_ids), dtype=np.float32)\n",
    "    max_pop = pop_counts.max()\n",
    "    for iid, count in pop_counts.items():\n",
    "        idx = item_to_idx.get(iid)\n",
    "        if idx is not None:\n",
    "            pop_scores[idx] = count / max_pop\n",
    "\n",
    "    # Popularity\n",
    "    pop_model = PopularityRecommender(item_col=ITEM_COL)\n",
    "    pop_model.fit(train_df)\n",
    "    metrics_pop = evaluate_model(pop_model, ground_truth, users_eval, ks=[5, 10, 20], known_items=known_items_map)\n",
    "    metrics_pop[\"model\"] = \"popularity\"\n",
    "    metrics_pop[\"svd\"] = svd_comp\n",
    "    all_results.append(metrics_pop)\n",
    "\n",
    "    # Hybrids\n",
    "    for alpha in tqdm(ALPHAS, desc=f\"Alphas (SVD={svd_comp})\"):\n",
    "        model = ContentHybridRecommender(\n",
    "            item_ids=item_ids,\n",
    "            item_to_idx=item_to_idx,\n",
    "            item_matrix=item_matrix,\n",
    "            pop_scores=pop_scores,\n",
    "            pop_ranking=pop_ranking,\n",
    "            user_col=USER_COL,\n",
    "            item_col=ITEM_COL,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        model.fit(train_df)\n",
    "        metrics = evaluate_model(model, ground_truth, users_eval, ks=[5, 10, 20], known_items=known_items_map)\n",
    "        metrics[\"model\"] = f\"hybrid_alpha_{alpha}\"\n",
    "        metrics[\"svd\"] = svd_comp\n",
    "        all_results.append(metrics)\n",
    "\n",
    "    # Feature-kNN\n",
    "    max_k = max(KNN_NEIGHBORS)\n",
    "    knn_cache = PrecomputedFeatureKNN(item_matrix=item_matrix, item_ids=item_ids, item_to_idx=item_to_idx, max_neighbors=max_k)\n",
    "    for n_nb in tqdm(KNN_NEIGHBORS, desc=f\"kNN (SVD={svd_comp})\"):\n",
    "        knn_cache.default_n_neighbors = min(n_nb, knn_cache.max_neighbors)\n",
    "        metrics_knn = evaluate_model(knn_cache, ground_truth, users_eval, ks=[5, 10, 20], known_items=known_items_map)\n",
    "        metrics_knn[\"model\"] = f\"feature_knn_{n_nb}\"\n",
    "        metrics_knn[\"svd\"] = svd_comp\n",
    "        all_results.append(metrics_knn)\n",
    "\n",
    "all_results_df = pd.concat(all_results)\n",
    "all_results_pivot = all_results_df.pivot_table(index=[\"model\", \"svd\"], columns=\"k\", values=[\"hit_rate\", \"recall\", \"ndcg\"])\n",
    "all_results_df.head(), all_results_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dde3a",
   "metadata": {},
   "source": [
    "## LightFM heavy sweep (optional, skip if unavailable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from lightfm import LightFM\n",
    "    from lightfm.data import Dataset as LFMDataset\n",
    "\n",
    "    lfm_ds = LFMDataset()\n",
    "    lfm_ds.fit(users=train_df[USER_COL].unique(), items=train_df[ITEM_COL].unique())\n",
    "    interactions, _ = lfm_ds.build_interactions(train_df[[USER_COL, ITEM_COL]].itertuples(index=False, name=None))\n",
    "\n",
    "    # Use first SVD setting (or None)\n",
    "    item_ids_lfm, item_to_idx_lfm, item_matrix_lfm = build_feature_matrix(\n",
    "        base_feats=base_feats,\n",
    "        meta=meta_filtered,\n",
    "        use_svd=USE_SVD,\n",
    "        svd_components=SVD_COMPONENTS[0] if USE_SVD else None,\n",
    "    )\n",
    "    lfm_item_features = csr_matrix(item_matrix_lfm)\n",
    "\n",
    "    user_id_map, user_feature_map, item_id_map, _ = lfm_ds.mapping()\n",
    "    inv_item_map = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "    for loss in LIGHTFM_LOSSES:\n",
    "        for factors in LIGHTFM_FACTORS:\n",
    "            for epochs in LIGHTFM_EPOCHS:\n",
    "                print(f\"LightFM loss={loss}, factors={factors}, epochs={epochs}\")\n",
    "                model_lfm = LightFM(loss=loss, no_components=factors, random_state=RANDOM_STATE)\n",
    "                model_lfm.fit(interactions, item_features=lfm_item_features, epochs=epochs, num_threads=4)\n",
    "\n",
    "                class LightFMWrapper:\n",
    "                    def recommend(self, user_id: int, known_items: List[int], k: int) -> List[int]:\n",
    "                        if user_id not in user_id_map:\n",
    "                            return []\n",
    "                        uid = user_id_map[user_id]\n",
    "                        scores = model_lfm.predict(uid, np.arange(len(inv_item_map)), item_features=lfm_item_features)\n",
    "                        ranked = np.argsort(-scores)\n",
    "                        recs = []\n",
    "                        known_set = set(known_items)\n",
    "                        for idx in ranked:\n",
    "                            itm = inv_item_map[idx]\n",
    "                            if itm in known_set:\n",
    "                                continue\n",
    "                            recs.append(itm)\n",
    "                            if len(recs) >= k:\n",
    "                                break\n",
    "                        return recs\n",
    "\n",
    "                lfm_wrapper = LightFMWrapper()\n",
    "                metrics_lfm = evaluate_model(lfm_wrapper, ground_truth, users_eval, ks=[5, 10, 20], known_items=known_items_map)\n",
    "                metrics_lfm[\"model\"] = f\"lightfm_{loss}_f{factors}_e{epochs}\"\n",
    "                metrics_lfm[\"svd\"] = SVD_COMPONENTS[0] if USE_SVD else None\n",
    "                all_results_df = pd.concat([all_results_df, metrics_lfm])\n",
    "                all_results_pivot = all_results_df.pivot_table(index=[\"model\", \"svd\"], columns=\"k\", values=[\"hit_rate\", \"recall\", \"ndcg\"])\n",
    "                display(metrics_lfm)\n",
    "except Exception as e:\n",
    "    print(\"LightFM not available or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bb57f",
   "metadata": {},
   "source": [
    "## Top models summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_k = 10\n",
    "metric = \"ndcg\"\n",
    "summary = (\n",
    "    all_results_df[all_results_df[\"k\"] == primary_k]\n",
    "    .sort_values(by=metric, ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"Top 5 models by NDCG@10:\")\n",
    "display(summary.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749749c7",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For full data, set `SAMPLE_USERS=None` and trim grids if needed.\n",
    "- BM25 + SVD + hybrid alphas are generally the strongest content baselines; kNN and LightFM are additional comparisons.\n",
    "- Use the HPC script for large runs; notebook is for experiments/reporting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
